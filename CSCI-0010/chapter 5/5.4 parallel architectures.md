# 5.4 Parallel Architectures
a problem with one processor can be solved in _n_ time
a problem with two processors can be solved with _n_/2 time
a problem with three processors can be solved with _n_/3 time

## Parallel Computing
**bit level**: increase word size
- on 8-bit processors, operations on 16-bit data values would require 2 operations, one for the upper 8 bits and one for the lower 8 bits
- increasing word size reduces the number of operations on data values larger than the word size (leading to 64-bit processors)

**instruction level**: some instructions in a program can be carried out independently in parallel
- ex) programs that do operations on unrelated data, like simultaneously adding 2 integers and multiplying 2 numbers on different ALUs
- superscalar: processors that exploit these situations and send instructions to different processor functional units
- contains several execution resources (_execution units_), not processors

**data level (synchronous processing)**: single set of instructions can be run on different data sets
- _single instructions, multiple data (SIMD)_: relies on central unit directing ALUs to carry the same operation
- ex) adjusting image brightness must add a value to one of several million pixels; this problem uses the same addition but different pixels or data sets

**task level**: different processors can execute different tasks on the same or different data set
- working on the same data set: pipelined von Neumann machine

![](..\..\.pastes\2021-06-29-19-10-49.png)

- different processors running different tasks on different data, requires shared memory to coordinate processes (**shared memory parallel processor**)

in data-level environments processors: each processor computes the grades for a different class
in task-level environments processors: each processor computes a grade for the same class 

![](..\..\.pastes\2021-06-29-19-14-24.png)

## Classes of Parallel Hardware
multicore processors: many independent cores (CPUs)
symmetric multiprocessors (SMPs): multiple identical cores
- bus creates shared memory
- limited to 32 cores

superscalar processors: many execution units
distributed computers: many memory units connected by networking
cluster: group of machines connected through a network
massively parallel processor: networked with +1000 processors 

modern systems are a combination of multiple classes of parallel hardware
- typical chips contain 2-8 cores that operate SMPs connected by a network into a cluster
- mixture of shared and distributed memory
- GPU can also be connected to multicore processors

science: emphasize data parallelism
search engine: task-level parallelism