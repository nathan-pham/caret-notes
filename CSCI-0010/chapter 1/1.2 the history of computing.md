# 1.2 The History of Computing
hardware and software history are discussed separately because they had different impacts on computer system design 

## A Brief History of Computing Hardware
devices assisting humans in computations are rooted in ancient history

### Early History
Stonehenge: a collection of monoliths in Great Britain thought to be an early temple, cemetery, calendar or astrological calculator
- Neolithic stone structure in the Salisbury Plain in England first erected beginning in 2180 BCE

_abacus_ (16th century): recorded numeric values and enabled more complex arithmetic 
gear-driven mechanical machines (mid 17th century): designed by French mathematician Blaise Pascal, performed whole-number addition and subtraction
- improved upon by Gottfried Wilhelm von Leibniz, who implemented all four whole-number operations (was not very reliable)

_Jacquard's loom_ (late 18th century): developed by Joseph Jacquard to weave colored cloth based on punched cards
- not a computing device but the first implementation of card input

_analytical engine_: theory conceptualized by British mathematician Charles Babbage
- first design to include memory (eliminated the need for re-entering intermediate values)
- leveraged number & mechanical step inputs with punched cards

Ada Augusta, Countess of Lovelace: daughter of Lord Byron and extended the analytical engine
- thought to be the first programmer
- described series of repeatable instructions 

mechanical adding machine (20th century): produced and sold by William Burroughs 
electro-mechanical tabulator: invented by Dr. Herman Hollerith, read information from punched cards
- revolutionized how the US census was taken
- Hollerith later created IBM

_Turing machine_ (1936): an abstract mathematical model invented by British mathematician Alan M. Turing
- became the foundation of modern computing
- Turing Award: the most prestigious award given in computer science

1-bit binary adder (1937): a device using relays to add binary digits, constructed by George Stibitz
symbolic logic (1938): paper on implementation with relays by Claude E. Shannon

mechanical binary computer (1938): first programmable machine developed by Konrad Zuse
Colossus (1943): first all-programmable electronic digital computer built in London by Thomas Flowers

IBM Automatic Sequence Controlled Calculator (_Harvard Mark I_) (1944)
ENIAC (1946) & EDVAC (1950): giant World War II-era computers worked on by John von Neumann
UNIVAC I (1951): first commercial computer used by the US Bureau of the Census
- used to predict the outcome of a presidential election

### Counting Precedes Writing 
7500 BCE farmers shaped clay counters 
- cones meant small measures of grain, spheres for large measures of grain, and a cylinder for an animal
- 8000 tokens have been found around Palestine, Anatolia, Syria, Mesopotamia & Iran

3500 BCE stamped clay balls were used as envelopes to hold tokens
3300 - 3200 BCE just used the impressions of tokens onto clay
3100 BCE styluses were used to draw tokens rather than 3D shapes 
writing was derived from counting as abstract numerals were created to represent goods

### First Generation (1951 - 1959)
commercial computers within the first generation used _vacuum tubes_ to store information
- generated too much heat & were unreliable 
- required heavy-duty air conditioning, large spaces & frequent maintenance 

_magnetic drum_: a memory device that rotated under a read/write head
the primary input device was a card reader that accepted a hole punched IBM card
- _magnetic tape drives_ are sequential storage units that increased in popularity because they were faster than card readers 

the primary output device was a punched card or a line printer 

_auxiliary storage devices_: external storage that isn't computer memory
_peripheral devices_: input devices, output devices, and auxiliary storage devices

### Second Generation (1959 - 1965)
_transistors_ replaced the vacuum tube
- faster, smaller, cheaper, durable
- developed by Nobel Prize winners John Bardeen, Walter H. Brattain, and William B. Shockley

_magnetic cores_: the second generation of memory capable of storing one bit in strung together in cells
- information was instantaneously available because the device was motionless & electronically based

_magnetic disk_: faster than magnetic tape because information could be referenced by location on the disk
- data organized into unique _addresses_ 
- read/write heads can be directed to a location on the disk where information is stored without accessing all of the previous information

### Third Generation (1965 - 1971)
_circuit boards_: hand printed sheets with transistors & computer components
_integrated circuits_ (ICs): silicon sheets containing transistors, components, and connections
- much smaller, cheaper, faster, and reliable than printed boards

_Moore's Law_: Gordon Moore (co-founder of Intel) noted the number of circuits within a single IC doubled each year
transistors were also used in memory boards, but auxiliary storage was still needed because transistors were _volatile_ and lost information when energy was lost
_terminal_: input/output with a keyboard and screen provided direct access to the computer and an immediate response 

### Fourth Generation (1971 - Present)
_Large-scale integration_: moved from a thousand transistors to entire microcomputers on a single chip 
- _personal computers_ (PC) are cheap enough for the average consumer
- Apple, Tandy/Radio Shack, Atari, Commodore, Sun, IBM, Remington, Rand, NCR, DEC, Hewlett-Packard, Control Data, and Burroughs enter the commercial market

Steve Wozniak and Steve Jobs create the Apple Computer, marketing a personal computer kit 
(1981) IBM PC was introduced
- Dell & Compaq create IBM compatible PCs

(1984) Apple releases Macintosh

_workstations_: business use computers networked together 
workstations were enhanced by RISC (reduced-instruction-set computer); computers were built to understand a native _machine language_
- IBM 370/168 implemented 200 specialized instructions
- (1987) Sun Microsystems implemented a _UNIX_ workstation with a RISC chip

Moore's law was modified to say chip power doubled or price halved every 18 months, with each generation of computer hardware more powerful, smaller, and cheaper than the previous

### Parallel Computing 
_parallel architectures_ rely on interconnected CPUs
- increase speed of execution but requires programmers to rewrite software initially designed for sequential machines
- _SIMD_ (single-instruction, multiple-data-stream): a given step in a program is separated and executed simultaneously
- _MIMD_ (multiple-instruction, multiple-data-stream): programs are separated and executed simultaneously

there are two main types of parallel architectures
1. all processors share the same memory
2. processors have local memory but are internally networked

### Networking
(1973) Robert Metcalfe & David Boggs invent the _Ethernet_, which set protocols and used a coaxial cable to allow separate machines to communicate
- (1979) DEC, Intel, and Xerox establish Ethernet as the new standard
- (1985) Intel releases an advanced chip capable of handling networking on personal computers

(1989) Novell's Netware connected PCs into a  _file server_
- enabled central control while giving individual machines autonomy 
- _LANs_ (local area networks): networked workstations or PCs

(1960s) ARPANET: government sponsored network with 11 nodes based in LA and Boston
- used _packet switching_ to allow messages to share lines
- (1990s) evolved into the _Internet_

The Internet is composed of several global networks that communicate through TCP/IP (Transmission Control Protocol/Internet Protocol)
- did not fully supplant Ethernet, which allowed for easy local connections in office and PCs

### Cloud Computing
_data center_: a third-party business that provides and configures the necessary hardware to maximize computing power for a business 
- resolves hardware issues regarding maintenance & upgrading
- often took days to establish communication between a server and computer needed by the business
- evolved into _cloud computing_, use virtual computer resources rather than physical devices

## A Brief History of Computing Software
hardware is directed by software, which has evolved over time into modern computing systems

### First-Generation Software (1951 - 1959)
_machine language_: instructions embedded into the electrical circuitry of a computer
- written in _binary_
- programmers needed to remember the sequence of numbers, which was tedious and error-prone

_assembly language_ : mnemonic codes that represented _machine language_ instructions
- _translators_: converted assembly into machine code
  - _assembler_: reads program instructions written in assembly and translates them into the machine language equivalent
- not human readable but much easier to use

_system programmers_: made tools to make programming easier for

### Second-Generation Software (1959 - 1965)
